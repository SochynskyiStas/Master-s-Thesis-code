{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_final_file.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>bin_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>something that i have been noticing recently i...</td>\n",
       "      <td>Catastrophizing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i went on vacation and just got back yesterday...</td>\n",
       "      <td>Catastrophizing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i've had crippling depression since i started ...</td>\n",
       "      <td>Jumping to conclusion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i feel like confusion is a huge part to depres...</td>\n",
       "      <td>Not distored</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i skipped both of my classes yesterday. i had ...</td>\n",
       "      <td>Not distored</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text              sentiment  \\\n",
       "0  something that i have been noticing recently i...        Catastrophizing   \n",
       "1  i went on vacation and just got back yesterday...        Catastrophizing   \n",
       "2  i've had crippling depression since i started ...  Jumping to conclusion   \n",
       "3  i feel like confusion is a huge part to depres...           Not distored   \n",
       "4  i skipped both of my classes yesterday. i had ...           Not distored   \n",
       "\n",
       "   bin_sentiment  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              0  \n",
       "4              0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distorted_only = df[df['bin_sentiment'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df_distorted_only['text'], df_distorted_only['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    train_size = 0.7, \n",
    "                                                    random_state = 42,\n",
    "                                                    stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOG + BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(analyzer = \"word\", \n",
    "                                   tokenizer = nltk.word_tokenize,\n",
    "                                   preprocessor = None, \n",
    "                                   stop_words = 'english', \n",
    "                                   min_df = 3)\n",
    "\n",
    "train_data_features = count_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stas\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "log_reg_BOW = LogisticRegression().fit(train_data_features, y_train)\n",
    "\n",
    "BOW_log_features = count_vectorizer.transform(X_test)\n",
    "y_pred = log_reg_BOW.predict(BOW_log_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      " Black and white thinking.     0.2200    0.2157    0.2178        51\n",
      "           Catastrophizing     0.2941    0.1724    0.2174        29\n",
      "Disqualifying the positive     0.2593    0.2059    0.2295        34\n",
      "       Emotional reasoning     0.2766    0.2407    0.2574        54\n",
      "     Jumping to conclusion     0.2143    0.3158    0.2553        57\n",
      "                  Labeling     0.2326    0.2857    0.2564        35\n",
      "       Over-generalization     0.1538    0.1463    0.1500        41\n",
      "           Personalization     0.2800    0.2258    0.2500        31\n",
      "         Should statements     0.2800    0.2800    0.2800        25\n",
      "\n",
      "                  accuracy                         0.2353       357\n",
      "                 macro avg     0.2456    0.2320    0.2349       357\n",
      "              weighted avg     0.2405    0.2353    0.2340       357\n",
      "\n",
      "[0.21568627 0.17241379 0.20588235 0.24074074 0.31578947 0.28571429\n",
      " 0.14634146 0.22580645 0.28      ]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred, digits = 4))\n",
    "\n",
    "#print('Confusion Matrix : \\n' + str(confusion_matrix(y_test,y_pred)))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print(cm.diagonal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid: log + BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done 286 tasks      | elapsed:   42.3s\n",
      "[Parallel(n_jobs=-1)]: Done 664 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:  2.4min finished\n"
     ]
    }
   ],
   "source": [
    "penalty = ['none', 'l1', 'l2', 'elasticnet']\n",
    "C_first = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000] # 1\n",
    "C_second = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "solver = ['liblinear', 'saga', 'newton-cg', 'lbfgs']\n",
    "\n",
    "param_grid = dict(penalty = penalty,\n",
    "                  C = C_second,\n",
    "                  solver = solver)\n",
    "\n",
    "grid = GridSearchCV(estimator = log_reg_BOW,\n",
    "                    param_grid = param_grid,\n",
    "                    scoring = 'f1_weighted', #'recall'; ‘accuracy’; ‘precision’\n",
    "                    verbose = 1,\n",
    "                    n_jobs = -1)\n",
    "grid_result = grid.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  0.2399485347355986\n",
      "Best Params:  {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "print('Best Score: ', grid_result.best_score_)\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First iteration\n",
    "Best Score:  0.2399485347355986\n",
    "Best Params:  {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n",
    "\n",
    "Second iteration\n",
    "Best Score:  0.2399485347355986\n",
    "Best Params:  {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_BOW = LogisticRegression(C = 1, penalty = \"l2\", solver = \"liblinear\").fit(train_data_features, y_train)\n",
    "\n",
    "BOW_log_features = count_vectorizer.transform(X_test)\n",
    "y_pred = log_reg_BOW.predict(BOW_log_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25770308123249297\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      " Black and white thinking.       0.25      0.27      0.26        51\n",
      "           Catastrophizing       0.29      0.17      0.22        29\n",
      "Disqualifying the positive       0.28      0.21      0.24        34\n",
      "       Emotional reasoning       0.31      0.28      0.29        54\n",
      "     Jumping to conclusion       0.24      0.37      0.29        57\n",
      "                  Labeling       0.24      0.26      0.25        35\n",
      "       Over-generalization       0.16      0.15      0.15        41\n",
      "           Personalization       0.27      0.23      0.25        31\n",
      "         Should statements       0.33      0.32      0.33        25\n",
      "\n",
      "                  accuracy                           0.26       357\n",
      "                 macro avg       0.26      0.25      0.25       357\n",
      "              weighted avg       0.26      0.26      0.26       357\n",
      "\n",
      "[0.2745098  0.17241379 0.20588235 0.27777778 0.36842105 0.25714286\n",
      " 0.14634146 0.22580645 0.32      ]\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print(cm.diagonal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOG + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vect = TfidfVectorizer(min_df = 3,\n",
    "                          tokenizer = nltk.word_tokenize,\n",
    "                          preprocessor = None, \n",
    "                          stop_words = 'english')\n",
    "\n",
    "TF_IDF_train_data_features = tf_vect.fit_transform(X_train)\n",
    "\n",
    "logreg_TF_IDF = LogisticRegression().fit(TF_IDF_train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_features_TF_IDF = tf_vect.transform(X_test)\n",
    "y_pred = logreg_TF_IDF.predict(data_features_TF_IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOG + TF-IDF Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  71 tasks      | elapsed:    5.4s\n",
      "[Parallel(n_jobs=-1)]: Done 353 tasks      | elapsed:   30.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  0.253568666162271\n",
      "Best Params:  {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:  1.2min finished\n"
     ]
    }
   ],
   "source": [
    "penalty = ['none', 'l1', 'l2', 'elasticnet']\n",
    "C_first = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000] # 1\n",
    "C_second = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "solver = ['liblinear', 'saga', 'newton-cg', 'lbfgs']\n",
    "\n",
    "param_grid = dict(penalty = penalty,\n",
    "                  C = C_second,\n",
    "                  solver = solver)\n",
    "\n",
    "grid = GridSearchCV(estimator = log_reg_BOW,\n",
    "                    param_grid = param_grid,\n",
    "                    scoring = 'f1_weighted', #'recall'; ‘accuracy’; ‘precision’\n",
    "                    verbose = 1,\n",
    "                    n_jobs = -1)\n",
    "grid_result = grid.fit(TF_IDF_train_data_features, y_train)\n",
    "\n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_BOW = LogisticRegression(C = 10, \n",
    "                                 penalty = \"l2\", \n",
    "                                 solver = \"liblinear\").fit(train_data_features, y_train)\n",
    "\n",
    "BOW_log_features = count_vectorizer.transform(X_test)\n",
    "y_pred = log_reg_BOW.predict(BOW_log_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22128851540616246\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      " Black and white thinking.       0.19      0.22      0.20        51\n",
      "           Catastrophizing       0.15      0.14      0.14        29\n",
      "Disqualifying the positive       0.21      0.21      0.21        34\n",
      "       Emotional reasoning       0.23      0.17      0.19        54\n",
      "     Jumping to conclusion       0.25      0.32      0.28        57\n",
      "                  Labeling       0.24      0.29      0.26        35\n",
      "       Over-generalization       0.16      0.15      0.15        41\n",
      "           Personalization       0.35      0.26      0.30        31\n",
      "         Should statements       0.26      0.24      0.25        25\n",
      "\n",
      "                  accuracy                           0.22       357\n",
      "                 macro avg       0.23      0.22      0.22       357\n",
      "              weighted avg       0.22      0.22      0.22       357\n",
      "\n",
      "[0.21568627 0.13793103 0.20588235 0.16666667 0.31578947 0.28571429\n",
      " 0.14634146 0.25806452 0.24      ]\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print(cm.diagonal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOG + doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(df_distorted_only)\n",
    "\n",
    "train_tagged = train_data.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['text']), \n",
    "                             tags=[r['sentiment']]), axis = 1)\n",
    "\n",
    "test_tagged = test_data.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['text']), \n",
    "                             tags=[r['sentiment']]), axis = 1)\n",
    "\n",
    "trainsent = train_tagged.values\n",
    "\n",
    "doc2vec_model = Doc2Vec(trainsent, \n",
    "                        vector_size = 300, \n",
    "                        window = 5, #The maximum distance between the current and predicted word within a sentence. \n",
    "                        workers = 4,\n",
    "                        epochs = 50,\n",
    "                        dm = 1)\n",
    "\n",
    "testsent = test_tagged.values\n",
    "y_train, X_train = zip(*[(doc.tags[0], doc2vec_model.infer_vector(doc.words, steps=20)) for doc in trainsent])\n",
    "y_test, X_test = zip(*[(doc.tags[0], doc2vec_model.infer_vector(doc.words, steps=20)) for doc in testsent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_doc2vec = LogisticRegression()\n",
    "logreg_doc2vec.fit(X_train, y_train)\n",
    "y_pred = logreg_doc2vec.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      " Black and white thinking.     0.2195    0.2000    0.2093        45\n",
      "           Catastrophizing     0.2381    0.2083    0.2222        24\n",
      "Disqualifying the positive     0.1600    0.1333    0.1455        30\n",
      "       Emotional reasoning     0.3158    0.3158    0.3158        38\n",
      "     Jumping to conclusion     0.2000    0.2075    0.2037        53\n",
      "                  Labeling     0.3077    0.4000    0.3478        30\n",
      "       Over-generalization     0.0800    0.0606    0.0690        33\n",
      "           Personalization     0.2143    0.2222    0.2182        27\n",
      "         Should statements     0.4000    0.5882    0.4762        17\n",
      "\n",
      "                  accuracy                         0.2391       297\n",
      "                 macro avg     0.2373    0.2596    0.2453       297\n",
      "              weighted avg     0.2271    0.2391    0.2310       297\n",
      "\n",
      "[0.2        0.20833333 0.13333333 0.31578947 0.20754717 0.4\n",
      " 0.06060606 0.22222222 0.58823529]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred, digits = 4))\n",
    "\n",
    "#print('Confusion Matrix : \\n' + str(confusion_matrix(y_test,y_pred)))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print(cm.diagonal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOG + doc2vec Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:   17.7s\n",
      "[Parallel(n_jobs=-1)]: Done 200 tasks      | elapsed:   58.1s\n",
      "[Parallel(n_jobs=-1)]: Done 450 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 640 out of 640 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  0.7534942492300717\n",
      "Best Params:  {'C': 0.5, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "penalty = ['none', 'l1', 'l2', 'elasticnet']\n",
    "C_first = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000] # 1\n",
    "C_second = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "C_third = [0.1, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2]\n",
    "solver = ['liblinear', 'saga', 'newton-cg', 'lbfgs']\n",
    "\n",
    "param_grid = dict(penalty = penalty,\n",
    "                  C = C_third,\n",
    "                  solver = solver)\n",
    "\n",
    "grid = GridSearchCV(estimator = logreg_doc2vec,\n",
    "                    param_grid = param_grid,\n",
    "                    scoring = 'f1_weighted', #'f1_weighted'; ‘macro’; ‘precision’\n",
    "                    verbose = 1,\n",
    "                    n_jobs = -1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_doc2vec = LogisticRegression(C = 1, \n",
    "                                 penalty = \"l2\", \n",
    "                                 solver = \"liblinear\")\n",
    "logreg_doc2vec.fit(X_train, y_train)\n",
    "y_pred = logreg_doc2vec.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      " Black and white thinking.     0.2174    0.2222    0.2198        45\n",
      "           Catastrophizing     0.1429    0.0833    0.1053        24\n",
      "Disqualifying the positive     0.1481    0.1333    0.1404        30\n",
      "       Emotional reasoning     0.2683    0.2895    0.2785        38\n",
      "     Jumping to conclusion     0.2241    0.2453    0.2342        53\n",
      "                  Labeling     0.2647    0.3000    0.2812        30\n",
      "       Over-generalization     0.1154    0.0909    0.1017        33\n",
      "           Personalization     0.2500    0.2593    0.2545        27\n",
      "         Should statements     0.3913    0.5294    0.4500        17\n",
      "\n",
      "                  accuracy                         0.2290       297\n",
      "                 macro avg     0.2247    0.2392    0.2295       297\n",
      "              weighted avg     0.2185    0.2290    0.2220       297\n",
      "\n",
      "[0.22222222 0.08333333 0.13333333 0.28947368 0.24528302 0.3\n",
      " 0.09090909 0.25925926 0.52941176]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred, digits = 4))\n",
    "\n",
    "#print('Confusion Matrix : \\n' + str(confusion_matrix(y_test,y_pred)))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print(cm.diagonal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_tfidf = SVC().fit(TF_IDF_train_data_features, y_train)\n",
    "data_features_TF_IDF = tf_vect.transform(X_test)\n",
    "y_pred = svc_tfidf.predict(data_features_TF_IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      " Black and white thinking.       0.23      0.31      0.26        51\n",
      "           Catastrophizing       0.00      0.00      0.00        29\n",
      "Disqualifying the positive       0.50      0.03      0.06        34\n",
      "       Emotional reasoning       0.23      0.46      0.31        54\n",
      "     Jumping to conclusion       0.20      0.56      0.30        57\n",
      "                  Labeling       0.80      0.23      0.36        35\n",
      "       Over-generalization       0.00      0.00      0.00        41\n",
      "           Personalization       0.50      0.06      0.11        31\n",
      "         Should statements       1.00      0.12      0.21        25\n",
      "\n",
      "                  accuracy                           0.24       357\n",
      "                 macro avg       0.38      0.20      0.18       357\n",
      "              weighted avg       0.34      0.24      0.20       357\n",
      "\n",
      "[0.31372549 0.         0.02941176 0.46296296 0.56140351 0.22857143\n",
      " 0.         0.06451613 0.12      ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stas\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "#print('Confusion Matrix : \\n' + str(confusion_matrix(y_test,y_pred)))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print(cm.diagonal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid:SVM + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   40.5s\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 960 out of 960 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  0.2458842219967091\n",
      "Best Params:  {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "C_second = [100, 200, 300, 400, 500, 600, 700, 800]\n",
    "gamma = [0.0001, 0.001, 0.01, 0.00001, 0.000001, 0.00000001]\n",
    "gamma_second = [0.01, 0.02, 0.03, 0.04, 0.05, 0.06]\n",
    "kernel = ['linear', 'poly', 'rbf', 'sigmoid'] #‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’\n",
    "\n",
    "param_grid = dict(C = C,\n",
    "                  gamma = gamma_second,\n",
    "                  kernel = kernel)\n",
    "\n",
    "grid = GridSearchCV(estimator = svc_tfidf,\n",
    "                    param_grid = param_grid,\n",
    "                    scoring = 'balanced_accuracy', #f1_weighted did not work\n",
    "                    verbose = 1,\n",
    "                    n_jobs = -1)\n",
    "grid_result = grid.fit(TF_IDF_train_data_features, y_train)\n",
    "\n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First iteration\n",
    "Best Score:  0.2458842219967091\n",
    "Best Params:  {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
    "\n",
    "- Second iteration (C_second + gamma)\n",
    "Best Score:  0.2458842219967091\n",
    "Best Params:  {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
    "\n",
    "- Third iteration (C_second + gamma_second)\n",
    "Best Score:  0.24945291462095726 - best\n",
    "Best Params:  {'C': 300, 'gamma': 0.006, 'kernel': 'sigmoid'}\n",
    "\n",
    "- 4th iteration (C + gamma_second)\n",
    "Best Score:  0.2458842219967091\n",
    "Best Params:  {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_tfidf = SVC(C = 300, \n",
    "                gamma = 0.006, kernel = 'sigmoid').fit(TF_IDF_train_data_features, y_train)\n",
    "data_features_TF_IDF = tf_vect.transform(X_test)\n",
    "y_pred = svc_tfidf.predict(data_features_TF_IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      " Black and white thinking.   0.186667  0.274510  0.222222        51\n",
      "           Catastrophizing   0.222222  0.137931  0.170213        29\n",
      "Disqualifying the positive   0.227273  0.147059  0.178571        34\n",
      "       Emotional reasoning   0.300000  0.444444  0.358209        54\n",
      "     Jumping to conclusion   0.210526  0.280702  0.240602        57\n",
      "                  Labeling   0.400000  0.285714  0.333333        35\n",
      "       Over-generalization   0.129032  0.097561  0.111111        41\n",
      "           Personalization   0.333333  0.193548  0.244898        31\n",
      "         Should statements   0.250000  0.120000  0.162162        25\n",
      "\n",
      "                  accuracy                       0.240896       357\n",
      "                 macro avg   0.251006  0.220163  0.224591       357\n",
      "              weighted avg   0.245841  0.240896  0.233240       357\n",
      "\n",
      "[0.2745098  0.13793103 0.14705882 0.44444444 0.28070175 0.28571429\n",
      " 0.09756098 0.19354839 0.12      ]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred, digits = 6))\n",
    "\n",
    "#print('Confusion Matrix : \\n' + str(confusion_matrix(y_test,y_pred)))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print(cm.diagonal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM + doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_doc2vec = SVC().fit(X_train, y_train)\n",
    "y_pred = SVM_doc2vec.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      " Black and white thinking.     0.2245    0.2444    0.2340        45\n",
      "           Catastrophizing     0.2857    0.0833    0.1290        24\n",
      "Disqualifying the positive     0.1765    0.1000    0.1277        30\n",
      "       Emotional reasoning     0.3659    0.3947    0.3797        38\n",
      "     Jumping to conclusion     0.2381    0.3774    0.2920        53\n",
      "                  Labeling     0.2667    0.5333    0.3556        30\n",
      "       Over-generalization     0.0000    0.0000    0.0000        33\n",
      "           Personalization     0.2000    0.1111    0.1429        27\n",
      "         Should statements     0.4286    0.1765    0.2500        17\n",
      "\n",
      "                  accuracy                         0.2458       297\n",
      "                 macro avg     0.2429    0.2245    0.2123       297\n",
      "              weighted avg     0.2339    0.2458    0.2227       297\n",
      "\n",
      "[0.24444444 0.08333333 0.1        0.39473684 0.37735849 0.53333333\n",
      " 0.         0.11111111 0.17647059]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred, digits = 4))\n",
    "\n",
    "#print('Confusion Matrix : \\n' + str(confusion_matrix(y_test,y_pred)))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print(cm.diagonal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid: SVM + doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   33.8s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 960 out of 960 | elapsed:  2.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score:  0.6910882404927561\n",
      "Best Params:  {'C': 100, 'gamma': 0.003, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "C_second = [100, 200, 300, 400, 500, 600, 700, 800]\n",
    "gamma = [0.0001, 0.001, 0.01, 0.00001, 0.000001, 0.00000001]\n",
    "gamma_second = [0.001, 0.002, 0.003, 0.004, 0.005, 0.006]\n",
    "kernel = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "param_grid = dict(C = C,\n",
    "                  gamma = gamma_second,\n",
    "                  kernel = kernel)\n",
    "\n",
    "grid = GridSearchCV(estimator = SVM_doc2vec,\n",
    "                    param_grid = param_grid,\n",
    "                    scoring = 'f1_weighted', #'f1_weighted'; ‘macro’; ‘precision’\n",
    "                    verbose = 1,\n",
    "                    n_jobs = -1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First iteration (C + gamma)\n",
    "Best Score f1 weigted: 0.6909424496218526  \n",
    "Best Params:  {'C': 100, 'gamma': 0.01, 'kernel': 'sigmoid'}\n",
    "\n",
    "- Second iteration (C_second + gamma):\n",
    "Best Score f1 weigted:  0.6946541766177363\n",
    "Best Params:  {'C': 800, 'gamma': 0.001, 'kernel': 'rbf'}\n",
    "\n",
    "- Third iteration (C_second + gamma_second)\n",
    "Best Score:  0.6950169040140672 - BEST\n",
    "Best Params:  {'C': 400, 'gamma': 0.002, 'kernel': 'rbf'}\n",
    "\n",
    "- 4th iteration (C + gamma_second)\n",
    "Best Score f1 weigted: 0.6942637273067429\n",
    "Best Params:  {'C': 100, 'gamma': 0.006, 'kernel': 'rbf'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_doc2vec = SVC(C = 400, gamma = 0.002, kernel = \"rbf\").fit(X_train, y_train)\n",
    "y_pred = SVM_doc2vec.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            precision    recall  f1-score   support\n",
      "\n",
      " Black and white thinking.     0.2308    0.2000    0.2143        45\n",
      "           Catastrophizing     0.1739    0.1667    0.1702        24\n",
      "Disqualifying the positive     0.0938    0.1000    0.0968        30\n",
      "       Emotional reasoning     0.2500    0.2895    0.2683        38\n",
      "     Jumping to conclusion     0.2128    0.1887    0.2000        53\n",
      "                  Labeling     0.2857    0.4000    0.3333        30\n",
      "       Over-generalization     0.0455    0.0303    0.0364        33\n",
      "           Personalization     0.3077    0.2963    0.3019        27\n",
      "         Should statements     0.3636    0.4706    0.4103        17\n",
      "\n",
      "                  accuracy                         0.2222       297\n",
      "                 macro avg     0.2182    0.2380    0.2257       297\n",
      "              weighted avg     0.2111    0.2222    0.2147       297\n",
      "\n",
      "[0.2        0.16666667 0.1        0.28947368 0.18867925 0.4\n",
      " 0.03030303 0.2962963  0.47058824]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred, digits = 4))\n",
    "\n",
    "#print('Confusion Matrix : \\n' + str(confusion_matrix(y_test,y_pred)))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print(cm.diagonal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
